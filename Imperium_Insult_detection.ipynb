{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Insults in Social Commentary\n",
    "\n",
    "### Problem Definition:\n",
    "\n",
    "The challenge is to detect if a comment from an online conversation can be considered insulting to another participant in the conversation. The idea is to create a generalizable single-class classifier which could operate in a near real-time mode. In this project I build a system that can detect whether or not any given comment is insulting by building a machine learning system.\n",
    "\n",
    "In addition to just building a classifier this project also test different classifier - **Linear & Non-Linear** and decide which might be the best classifier for the purpose.\n",
    "\n",
    "\n",
    "Ref: https://www.kaggle.com/c/detecting-insults-in-social-commentary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# to split data\n",
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector machine\n",
    "from sklearn import svm\n",
    "# Naive Bayes\n",
    "import sklearn.naive_bayes as nb\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# random forest \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Loading\n",
    "\n",
    "Data for training and testing is obtained from Kaggle. The data set contains 3947 examples, each of which consists of the text of a particular post and its desired label. A label of 1 represents an insulting post, while a label of 0 represents a non-insulting post. \n",
    "\n",
    "For Example, \n",
    "\n",
    "**Text:** “You’re a moron, truth is beyond your reach”, Label: 1\n",
    "\n",
    "**Text:** “I’ll take that temp…I really hate the heat”, Label: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training file with pandas.\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>\"@SDL OK, but I would hope they'd sign him to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>\"Yeah and where are you now?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620015140Z</td>\n",
       "      <td>\"@jdstorm dont wish him injury but it happened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>20120530044519Z</td>\n",
       "      <td>\"Be careful,Jimbo.OG has a fork with your name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@tonnyb  Or they just don't pay attention \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Hmmm. Perhaps some who are too pig-faced to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@barrettmarson Huh? Her income was $21,912. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>20120611090207Z</td>\n",
       "      <td>\"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320162532Z</td>\n",
       "      <td>\"You with the 'racist' screen name\\n\\nYou are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Oh! &amp; cheating as well...1966 cup was a cheat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320075347Z</td>\n",
       "      <td>\"your such a dickhead...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20120320203947Z</td>\n",
       "      <td>\"Your a retard go post your head up your #%&amp;*\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612051612Z</td>\n",
       "      <td>\"@EephusBlue\\xa0Makes you want to say \"Mike Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>20120611234235Z</td>\n",
       "      <td>\"http://www.youtube.com/watch?v=tLYLLPHKRU4\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>20120502215304Z</td>\n",
       "      <td>\"And you know they've burned holes in all the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503031721Z</td>\n",
       "      <td>\"you are a land creature. You would drown....\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"To all those doom-and-gloomers, the chicken-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612144331Z</td>\n",
       "      <td>\"Craig, we have been saying from the start tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>20120529025329Z</td>\n",
       "      <td>\"Strange you are talking about that when there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612052926Z</td>\n",
       "      <td>\"But how would you actually get the key out?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528193950Z</td>\n",
       "      <td>\"oh cool, some more internet points for your c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>20120529230121Z</td>\n",
       "      <td>\"Even though I think there are better coaches ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619192544Z</td>\n",
       "      <td>\"No shit!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>0</td>\n",
       "      <td>20120610042233Z</td>\n",
       "      <td>\"clark just needs to stop pushing and pulling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>1</td>\n",
       "      <td>20120610154957Z</td>\n",
       "      <td>\"faggot\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>1</td>\n",
       "      <td>20120530140143Z</td>\n",
       "      <td>\"You really should be like the other Lemmings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>0</td>\n",
       "      <td>20120609221726Z</td>\n",
       "      <td>\"by your logic also legalize Heroin.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"The G.O.P. can cry all they want but ther the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@mikieboy - In a nutshell :-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>1</td>\n",
       "      <td>20120530010840Z</td>\n",
       "      <td>\"@Crissa:disqus LaRaza (The Race), NBP. Nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Hey, if the Republicans are determined to sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>0</td>\n",
       "      <td>20120527153826Z</td>\n",
       "      <td>\"Nah. Good folks there, good music POURS from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619003831Z</td>\n",
       "      <td>\"Hey nutbag..youre a traitor....\\n\\nObama has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>1</td>\n",
       "      <td>20120610125954Z</td>\n",
       "      <td>\"You're avatar is fail. Loser.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619022522Z</td>\n",
       "      <td>\"Why don't you shut your stupid mouth, pennyla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>0</td>\n",
       "      <td>20120529160326Z</td>\n",
       "      <td>\"We didn't even know who the rebels really are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618222326Z</td>\n",
       "      <td>\"You goofy Repub thugs don't have a clue, not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>0</td>\n",
       "      <td>20120527154034Z</td>\n",
       "      <td>\"socaljim:&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Nope, Kelly shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@Ritual Reality No I am simply saying that ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>1</td>\n",
       "      <td>20120610083604Z</td>\n",
       "      <td>\"True, maybe he's deliberately being an assh*l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>1</td>\n",
       "      <td>20120515160649Z</td>\n",
       "      <td>\"JoelWeltman, you look like a pedophile from y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>0</td>\n",
       "      <td>20120529001033Z</td>\n",
       "      <td>\"your a little high on the points but I do thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3937</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619145323Z</td>\n",
       "      <td>\"Your Yellowstone Fly Fishing Report:\\n\\n.. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612001129Z</td>\n",
       "      <td>\"MrO,\\n\\nProof is shown by liberals not wantin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619205630Z</td>\n",
       "      <td>\"The only ignorant person here is you, who thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3940</th>\n",
       "      <td>0</td>\n",
       "      <td>20120610114639Z</td>\n",
       "      <td>\"oh i had many cars like this before.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@Sara Besleaga Griji, doruri sau dorin\\\\xc8\\\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3947 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "0          1  20120618192155Z   \n",
       "1          0  20120528192215Z   \n",
       "2          0              NaN   \n",
       "3          0              NaN   \n",
       "4          0  20120619094753Z   \n",
       "5          0  20120620171226Z   \n",
       "6          0  20120503012628Z   \n",
       "7          1              NaN   \n",
       "8          1  20120502173553Z   \n",
       "9          1  20120620160512Z   \n",
       "10         0  20120620015140Z   \n",
       "11         0  20120530044519Z   \n",
       "12         0              NaN   \n",
       "13         0              NaN   \n",
       "14         0              NaN   \n",
       "15         1  20120611090207Z   \n",
       "16         1  20120320162532Z   \n",
       "17         0              NaN   \n",
       "18         1  20120320075347Z   \n",
       "19         1  20120320203947Z   \n",
       "20         0  20120612051612Z   \n",
       "21         0  20120611234235Z   \n",
       "22         0  20120502215304Z   \n",
       "23         0  20120503031721Z   \n",
       "24         0              NaN   \n",
       "25         0  20120612144331Z   \n",
       "26         0  20120529025329Z   \n",
       "27         0  20120612052926Z   \n",
       "28         0  20120528193950Z   \n",
       "29         0  20120529230121Z   \n",
       "...      ...              ...   \n",
       "3917       0  20120619192544Z   \n",
       "3918       0  20120610042233Z   \n",
       "3919       1  20120610154957Z   \n",
       "3920       1  20120530140143Z   \n",
       "3921       0  20120609221726Z   \n",
       "3922       0              NaN   \n",
       "3923       0              NaN   \n",
       "3924       1  20120530010840Z   \n",
       "3925       0              NaN   \n",
       "3926       0  20120527153826Z   \n",
       "3927       1  20120619003831Z   \n",
       "3928       1  20120610125954Z   \n",
       "3929       1  20120619022522Z   \n",
       "3930       0  20120529160326Z   \n",
       "3931       1  20120618222326Z   \n",
       "3932       0  20120527154034Z   \n",
       "3933       0              NaN   \n",
       "3934       1  20120610083604Z   \n",
       "3935       1  20120515160649Z   \n",
       "3936       0  20120529001033Z   \n",
       "3937       0  20120619145323Z   \n",
       "3938       0  20120612001129Z   \n",
       "3939       0  20120619205630Z   \n",
       "3940       0  20120610114639Z   \n",
       "3941       0              NaN   \n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "0                                  \"You fuck your dad.\"  \n",
       "1     \"i really don't understand your point.\\xa0 It ...  \n",
       "2     \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  \n",
       "3     \"listen if you dont wanna get married to a man...  \n",
       "4     \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  \n",
       "5     \"@SDL OK, but I would hope they'd sign him to ...  \n",
       "6                         \"Yeah and where are you now?\"  \n",
       "7     \"shut the fuck up. you and the rest of your fa...  \n",
       "8     \"Either you are fake or extremely stupid...may...  \n",
       "9     \"That you are an idiot who understands neither...  \n",
       "10    \"@jdstorm dont wish him injury but it happened...  \n",
       "11    \"Be careful,Jimbo.OG has a fork with your name...  \n",
       "12         \"@tonnyb  Or they just don't pay attention \"  \n",
       "13    \"Hmmm. Perhaps some who are too pig-faced to g...  \n",
       "14    \"@barrettmarson Huh? Her income was $21,912. H...  \n",
       "15    \"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...  \n",
       "16    \"You with the 'racist' screen name\\n\\nYou are ...  \n",
       "17    \"Oh! & cheating as well...1966 cup was a cheat...  \n",
       "18                            \"your such a dickhead...\"  \n",
       "19       \"Your a retard go post your head up your #%&*\"  \n",
       "20    \"@EephusBlue\\xa0Makes you want to say \"Mike Ma...  \n",
       "21         \"http://www.youtube.com/watch?v=tLYLLPHKRU4\"  \n",
       "22    \"And you know they've burned holes in all the ...  \n",
       "23       \"you are a land creature. You would drown....\"  \n",
       "24    \"To all those doom-and-gloomers, the chicken-l...  \n",
       "25    \"Craig, we have been saying from the start tha...  \n",
       "26    \"Strange you are talking about that when there...  \n",
       "27        \"But how would you actually get the key out?\"  \n",
       "28    \"oh cool, some more internet points for your c...  \n",
       "29    \"Even though I think there are better coaches ...  \n",
       "...                                                 ...  \n",
       "3917                                         \"No shit!\"  \n",
       "3918  \"clark just needs to stop pushing and pulling ...  \n",
       "3919                                           \"faggot\"  \n",
       "3920  \"You really should be like the other Lemmings ...  \n",
       "3921              \"by your logic also legalize Heroin.\"  \n",
       "3922  \"The G.O.P. can cry all they want but ther the...  \n",
       "3923                    \"@mikieboy - In a nutshell :-)\"  \n",
       "3924  \"@Crissa:disqus LaRaza (The Race), NBP. Nation...  \n",
       "3925  \"Hey, if the Republicans are determined to sta...  \n",
       "3926  \"Nah. Good folks there, good music POURS from ...  \n",
       "3927  \"Hey nutbag..youre a traitor....\\n\\nObama has ...  \n",
       "3928                    \"You're avatar is fail. Loser.\"  \n",
       "3929  \"Why don't you shut your stupid mouth, pennyla...  \n",
       "3930  \"We didn't even know who the rebels really are...  \n",
       "3931  \"You goofy Repub thugs don't have a clue, not ...  \n",
       "3932  \"socaljim:<div><br></div><div>Nope, Kelly shou...  \n",
       "3933  \"@Ritual Reality No I am simply saying that ba...  \n",
       "3934  \"True, maybe he's deliberately being an assh*l...  \n",
       "3935  \"JoelWeltman, you look like a pedophile from y...  \n",
       "3936  \"your a little high on the points but I do thi...  \n",
       "3937  \"Your Yellowstone Fly Fishing Report:\\n\\n.. Th...  \n",
       "3938  \"MrO,\\n\\nProof is shown by liberals not wantin...  \n",
       "3939  \"The only ignorant person here is you, who thi...  \n",
       "3940             \"oh i had many cars like this before.\"  \n",
       "3941  \"@Sara Besleaga Griji, doruri sau dorin\\\\xc8\\\\...  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  \n",
       "\n",
       "[3947 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect complete dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3947, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insult</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Comment\n",
       "Insult         \n",
       "0          2898\n",
       "1          1049"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the Count of Positive and Negative\n",
    "df[['Comment','Insult']].groupby('Insult').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, **1049** of the examples are labelled as **“insulting”**, while the remaining **2898** examples are labelled as **“not insulting”**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult                                            Comment\n",
       "3942       1  \"you are both morons and that is never happening\"\n",
       "3943       0  \"Many toolbars include spell check, like Yahoo...\n",
       "3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...\n",
       "3945       0  \"How about Felix? He is sure turning into one ...\n",
       "3946       0  \"You're all upset, defending this hipster band..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data\n",
    "df[['Insult', 'Comment']].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Wrangling\n",
    "\n",
    "Once the data has been loaded, data wrangling pipeline is being created to clean data and remove garbage characters. \n",
    "\n",
    "1. Text Cleansing.\n",
    "2. Tokenization\n",
    "3. Stemming\n",
    "4. Lemmatization\n",
    "5. Stop words removal\n",
    "6. Rare words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting Labels and comments\n",
    "label, comments = df['Insult'],df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking label, comments\n",
    "type(label), type(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Text Cleaning: To Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting data to lowercase\n",
    "comments=comments.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"@sdl ok, but i would hope they\\'d sign him to a one-year contract to start with. give him the chance to be reliable and productive, but give themselves the out if all his time off has hurt his playing skills or if he falls back into old habits.\"'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import re\n",
    "from re import sub\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning: Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_char(x):\n",
    "    # this will replace all punctuations with spaces\n",
    "    punc = string.punctuation.replace(\"-\", \"\")\n",
    "    punc = punc.replace(\"'\", \"\")\n",
    "    pat= r\"[{}]\".format(punc)\n",
    "    x=re.sub(pat, \" \", x)\n",
    "    \n",
    "    # this will replace all digits with None\n",
    "    x=re.sub(pattern=r\"\\d\", repl=r\" \", string=x)\n",
    "    \n",
    "    # this will strip extra white spaces\n",
    "    return \" \".join( i for i in stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(x):\n",
    "    #st = LancasterStemmer()\n",
    "    st = WordNetLemmatizer()\n",
    "    words=x.strip().split()\n",
    "    st3=SnowballStemmer(\"english\")\n",
    "    return [st3.stem(st.lemmatize(x)) for x in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making a copy of transformed \n",
    "comments_tran = comments.apply(remove_special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"i really don\\'t understand your point.\\\\xa0 it seems that you are mixing apples and oranges.\"',\n",
       " \"i realli don't understand your point \\\\xa it seem that you are mix appl and orang\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the change\n",
    "comments.iloc[1], comments_tran.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set, 153)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# getting stopwords for classifier\n",
    "stopWords = set(stopwords.words('english'))\n",
    "type(stopWords), len(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3947, 11775)\n"
     ]
    }
   ],
   "source": [
    "tf = text.TfidfVectorizer(stop_words=stopWords, ngram_range=(1, 1))\n",
    "X = tf.fit_transform(comments_tran)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is experimental tokenizer with ngram features\n",
    "# tf = text.TfidfVectorizer(stop_words=stopWords,ngram_range=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sample has ~0.13% non-zero features.\n"
     ]
    }
   ],
   "source": [
    "# checking the sparsity of matrix\n",
    "print(\"Each sample has ~{0:.2%} non-zero features.\".format(X.nnz / float(X.shape[0] * X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11775"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing words in vocab list\n",
    "vocab=list((tf.vocabulary_).keys())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaaaaaaaa', 'aaaah', 'aaahhh', 'aac', 'aamir', 'aap', 'aarongmy', 'ab', 'abacha', 'abandon', 'abc', 'abe', 'abel', 'aberdeen', 'abet', 'abid', 'abigail', 'abil', 'ability', 'abit', 'abl', 'abnorm', 'abolish', 'abolit', 'abomin', 'abort', 'abortifaci', 'abortion', 'abov', 'abraham', 'abroad', 'abrupt', 'abscam', 'absenc', 'absolut', 'absolutejok', 'absolutely', 'abstain', 'abstractfirework', 'absurd', 'absurdum', 'absurt', 'aburrido', 'abus', 'abuse', 'abuses', 'abxxv', 'abysm', 'ac', 'academ']\n"
     ]
    }
   ],
   "source": [
    "vocab.sort()\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Building Model\n",
    "\n",
    "I started out by trying linear classification models **Naïve Bayes, SVMs, and Logistic Regression** for building the classifier as they are very common and easy models for classification. \n",
    "\n",
    "Following this I built Non-Linear classifiers  like **Random Forest Classifier, Decision Tree Classifier**. Evnetually, I compare how these classifiers perform. \n",
    "\n",
    "I have implemented each of these classifiers with only unigrams features, followed by basic preprocessing (such as lowercasing all letters, removing punctuation and stemming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data - Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, X_val,label_train, label_val) = cv.train_test_split(X, label, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3157, 11775), (790, 11775))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# investigating the shape of data\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression (LR)\n",
    "\n",
    "ref: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_LR():\n",
    "    # creating classifier\n",
    "    clf = LogisticRegression(tol=1e-8, penalty='l2', C=2)\n",
    "    # training classifier\n",
    "    clf.fit(X_train, label_train)\n",
    "    # model type\n",
    "    print(\"Model: \",type(clf))\n",
    "    # Predicting probabilities\n",
    "    p = clf.predict_proba(X_val)\n",
    "    return (clf.predict(X_val),p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machine (SVM)\n",
    "\n",
    "ref: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_SVM():\n",
    "    # creating classifier\n",
    "    clf = svm.LinearSVC(penalty='l2', loss='squared_hinge',tol=1e-8)\n",
    "    # training classifier\n",
    "    clf.fit(X_train, label_train)\n",
    "    # model type\n",
    "    print(\"Model: \",type(clf))\n",
    "    return clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes (NB)\n",
    "\n",
    "ref: http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bernoulli Naive Baiyes\n",
    "def model_BernoulliNB():\n",
    "    # creating classifier\n",
    "    clf = nb.BernoulliNB(alpha=1.0, binarize=0.0)\n",
    "    # training classifier\n",
    "    clf.fit(X_train, label_train)\n",
    "    # model type\n",
    "    print(\"Model: \",type(clf))\n",
    "    # Predicting probabilities\n",
    "    p = clf.predict_proba(X_val)\n",
    "    return (clf.predict(X_val),p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Classification Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "def model_RF():\n",
    "    # creating classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    # training classifier\n",
    "    clf.fit(X_train, label_train)\n",
    "    # model type\n",
    "    print(\"Model: \",type(clf))\n",
    "    # Predicting probabilities\n",
    "    p = clf.predict_proba(X_val)\n",
    "    return (clf.predict(X_val),p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Classifier (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_DT():\n",
    "    # creating classifier\n",
    "    clf = DecisionTreeClassifier(max_depth=100)\n",
    "    # training classifier\n",
    "    clf.fit(X_train, label_train)\n",
    "    # model type\n",
    "    print(\"Model: \",type(clf))\n",
    "    # Predicting probabilities\n",
    "    p = clf.predict_proba(X_val)\n",
    "    return (clf.predict(X_val),p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Model Evaluation\n",
    "\n",
    "The primary evaluation metrics that I have used on my system training are **accuracy and 10-fold cross validation**. I used the training accuracy to determine how well the model is fitting in training data and testing with validation data. The cross validation accuracy was more significant because it was more generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC and AUC score\n",
    "from sklearn.metrics import roc_auc_score as auc_score\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_evaluation(model,label_test):\n",
    "    #accuracy=np.mean(model == label_test)\n",
    "    #print(\"%.4f\"%np.mean(model == label_test))\n",
    "    # confusion matrix:\n",
    "    cm = confusion_matrix(label_test, model, labels=None, sample_weight=None)\n",
    "    tp, fn, fp, tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    precision= float(tp)/(tp+fp)\n",
    "    recall =  float(tp)/(tp+tn)\n",
    "    accuracy = np.mean(model == label_test)\n",
    "    print_results (precision, recall, accuracy)\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def print_results (precision, recall, accuracy):\n",
    "    banner = \"Here is the classification report\"\n",
    "    print ('\\n',banner)\n",
    "    print ('=' * len(banner))\n",
    "    print ('{0:10s} {1:.1f}'.format('Precision',precision*100))\n",
    "    print ('{0:10s} {1:.1f}'.format('Recall',recall*100))\n",
    "    print ('{0:10s} {1:.1f}'.format('Accuracy',accuracy*100))\n",
    "    \n",
    "    \n",
    "    #print(\"*****PRECISION****\")\n",
    "    #print(\"%.4f\"%(tp/(tp+fp)))\n",
    "    #print(\"*****RECALL****\")\n",
    "    #print(\"%.4f\"%(tp/(tp+tn)))\n",
    "    #return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      " Here is the classification report\n",
      "=================================\n",
      "Precision  73.4\n",
      "Recall     98.1\n",
      "Accuracy   71.9\n",
      "AUC Score  52.1\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "clf_LR, p = model_LR()\n",
    "# model evaluation\n",
    "acc_LR = model_evaluation(clf_LR, label_val)\n",
    "\n",
    "print ('{0:10s} {1:.1f}'.format('AUC Score',auc_score(label_val, p[:,1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'sklearn.svm.classes.LinearSVC'>\n",
      "\n",
      " Here is the classification report\n",
      "=================================\n",
      "Precision  73.0\n",
      "Recall     91.9\n",
      "Accuracy   64.2\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "clf_SVM = model_SVM()\n",
    "# model evaluation\n",
    "acc_SVM = model_evaluation(clf_SVM, label_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "\n",
      " Here is the classification report\n",
      "=================================\n",
      "Precision  72.0\n",
      "Recall     98.7\n",
      "Accuracy   67.8\n",
      "AUC Score  41.3\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "clf_NB,p=model_BernoulliNB()\n",
    "# model evaluation\n",
    "acc_NB = model_evaluation(clf_NB, label_val)\n",
    "\n",
    "print ('{0:10s} {1:.1f}'.format('AUC Score',auc_score(label_val, p[:,1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "\n",
      " Here is the classification report\n",
      "=================================\n",
      "Precision  73.6\n",
      "Recall     90.5\n",
      "Accuracy   64.2\n",
      "AUC Score  46.6\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "clf_RF,p=model_RF()\n",
    "# model evaluation\n",
    "acc_RF = model_evaluation(clf_RF, label_val)\n",
    "\n",
    "print ('{0:10s} {1:.1f}'.format('AUC Score',auc_score(label_val, p[:,1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "\n",
      " Here is the classification report\n",
      "=================================\n",
      "Precision  73.5\n",
      "Recall     94.4\n",
      "Accuracy   68.1\n",
      "AUC Score  51.4\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "clf_DT,p=model_DT()\n",
    "# model evaluation\n",
    "acc_DT = model_evaluation(clf_DT, label_val)\n",
    "\n",
    "print ('{0:10s} {1:.1f}'.format('AUC Score',auc_score(label_val, p[:,1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy for all Models\n",
    "accuracy_normal=[acc_LR, acc_SVM, acc_NB, acc_RF, acc_DT]\n",
    "accuracy_normal=[('{0:2f}'.format(i*100)) for i in accuracy_normal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['71.139241', '63.544304', '67.848101', '64.177215', '68.101266']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy : 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "clf1 = LogisticRegression(tol=1e-8, penalty='l2', C=2)\n",
    "# Support Vector Machines\n",
    "clf2 = svm.LinearSVC(penalty='l2', loss='squared_hinge')\n",
    "# Naive Bayes\n",
    "clf3 = nb.BernoulliNB(alpha=1.0, binarize=0.0)\n",
    "# Random Forest\n",
    "clf4 = RandomForestClassifier(n_estimators=100)\n",
    "# Decision Tree\n",
    "clf5 = DecisionTreeClassifier(max_depth=100)\n",
    "\n",
    "models=[clf1, clf2, clf3, clf4, clf5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   82.1\n",
      "Accuracy   81.9\n",
      "Accuracy   74.6\n",
      "Accuracy   81.7\n",
      "Accuracy   78.3\n",
      "Normal Accuracy\n",
      "================\n",
      "['71.139241', '63.544304', '67.848101', '64.177215', '68.101266', '68.101266']\n",
      "Accuracy post CV\n",
      "================\n",
      "['82.1', '81.9', '74.6', '81.7', '78.3']\n"
     ]
    }
   ],
   "source": [
    "n_Folds = 10\n",
    "# Accuracy after cross validation:\n",
    "accuracy_cv=[]\n",
    "for clf in models:\n",
    "    accuracy_common=0\n",
    "    for test_run in range(n_Folds):\n",
    "        (X_train, X_test, y_train, y_test) = cv.train_test_split(X, label, test_size=.2)\n",
    "        # call classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "        model=clf.predict(X_test)\n",
    "        # compare result\n",
    "        accuracy=np.mean(model == y_test)\n",
    "        # append to common\n",
    "        accuracy_common += accuracy\n",
    "        # final score\n",
    "    print ('{0:10s} {1:.1f}'.format('Accuracy',float(accuracy_common)/10*100))\n",
    "    accuracy_cv.append('{0:.1f}'.format(float(accuracy_common)/10*100))\n",
    "    \n",
    "print(\"Normal Accuracy\")\n",
    "print(\"================\")\n",
    "print(accuracy_normal)\n",
    "print(\"Accuracy post CV\")\n",
    "print(\"================\")\n",
    "print(accuracy_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Results\n",
    "\n",
    "With all of the above features and techniques implemented, a cross-validation accuracy of 82.8% was achieved for **Support Vector Machine** Classifier. This was followed by **Random Forest Classifier and Logistic Regression** Respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
